{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Action_Recognition_I3D_FranciscoMena.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDq0CIKc1vO_"
      },
      "source": [
        "# Action Recognition with an Inflated 3D CNN (using tf hub)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIdAKAdELPSl"
      },
      "source": [
        "## Instrucciones Generales\n",
        "\n",
        "El siguiente práctico es **individual**. El formato de entregar es el **archivo .ipynb con todas las celdas ejecutadas**. Todas las preguntas deben ser respondida en celdas de texto. No se aceptará el _output_ de una celda de código como respuesta.\n",
        "\n",
        "**Nombre:** FRANCISCO MENA\n",
        "\n",
        "**Fecha de entrega: Abril 28 de 2021.**\n",
        "\n",
        "El siguiente práctico cuanta con varias secciones y al final 1 o más actividades a realizar. Algunas actividades correspondrán a escribir código y otras a responder preguntas. \n",
        "\n",
        "**Importante.** Para facilitar su ejecución, cada sección puede ser ejecutada independientemente.\n",
        "\n",
        "Se recomienda **fuertemente** revisar las secciones donde se entrega código porque algunas actividades de código pueden reutilizar el mismo código pero con cambios en algunas líneas.\n",
        "\n",
        "El practico debe entregarse de forma **Individual** en caso contrario obtendrán la mínima calificación (1). Asimismo, debe indicar su nombre donde se indica sino la práctica no será calificada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6W3FhoP3TxC"
      },
      "source": [
        "## 1.0 Introduction\n",
        "\n",
        "The goal of human activity recognition is to examine activities from video sequences or still images. One of the more important models of this area is described in the paper \"[Quo Vadis, Action Recognition? A New\n",
        "Model and the Kinetics Dataset](https://arxiv.org/abs/1705.07750)\" by Joao\n",
        "Carreira and Andrew Zisserman. The paper was published as a CVPR 2017 conference paper.\n",
        "\n",
        "\"Quo Vadis\" paper introduced a new architecture for video classification, the Inflated 3D Convnet or I3D. This architecture achieved state-of-the-art results on the UCF101 and HMDB51 datasets from fine-tuning these models. I3D models pre-trained on Kinetics\n",
        "also placed first in the CVPR 2017 [Charades challenge](http://vuchallenge.org/charades.html).\n",
        "\n",
        "The original module was trained on the [kinetics-400 dateset](https://deepmind.com/research/open-source/open-source-datasets/kinetics/)\n",
        "and knows about 400 different actions.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://neilyongyangnie.files.wordpress.com/2018/08/model-2.png?w=1088' width=\"900\" />\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In this tutorial we will use the I3D model to recognize activites in videos from a UCF101 dataset and own data. We will use TensorFlow Hub because it allows using the pre-trained model in an easy way.\n",
        "\n",
        "Based on: https://tfhub.dev/deepmind/i3d-kinetics-400/1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_0xc2jyNGRp"
      },
      "source": [
        "## 2.0 Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOHMWsFnITdi"
      },
      "source": [
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USf0UvkYIlKo"
      },
      "source": [
        "# TensorFlow and TF-Hub modules.\n",
        "from absl import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed\n",
        "\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "# Some modules to help with reading the UCF101 dataset.\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import tempfile\n",
        "import ssl\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio\n",
        "from IPython import display\n",
        "\n",
        "from urllib import request  # requires python3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY0hiqb_LKdY"
      },
      "source": [
        "### 2.1 Defining utilities to retrieve the UCF101 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRRa5RYGLogB"
      },
      "source": [
        "The following functions allow to fecth videos of the UCF101 dataset from the original repository. \n",
        "\n",
        "`list_ucf_videos` returns the list of the videos.\n",
        "`fetch_ucf_video` returns one selected video, you need to give the name of the video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMMS3TGdws7"
      },
      "source": [
        "# Utilities to fetch videos from UCF101 dataset\n",
        "UCF_ROOT = \"https://www.crcv.ucf.edu/THUMOS14/UCF101/UCF101/\"\n",
        "_VIDEO_LIST = None\n",
        "_CACHE_DIR = tempfile.mkdtemp()\n",
        "# As of July 2020, crcv.ucf.edu doesn't use a certificate accepted by the\n",
        "# default Colab environment anymore.\n",
        "unverified_context = ssl._create_unverified_context()\n",
        "\n",
        "def list_ucf_videos():\n",
        "  \"\"\"Lists videos available in UCF101 dataset.\"\"\"\n",
        "  global _VIDEO_LIST\n",
        "  if not _VIDEO_LIST:\n",
        "    index = request.urlopen(UCF_ROOT, context=unverified_context).read().decode(\"utf-8\")\n",
        "    videos = re.findall(\"(v_[\\w_]+\\.avi)\", index)\n",
        "    _VIDEO_LIST = sorted(set(videos))\n",
        "  return list(_VIDEO_LIST)\n",
        "\n",
        "def fetch_ucf_video(video):\n",
        "  \"\"\"Fetchs a video and cache into local filesystem.\"\"\"\n",
        "  cache_path = os.path.join(_CACHE_DIR, video)\n",
        "  if not os.path.exists(cache_path):\n",
        "    urlpath = request.urljoin(UCF_ROOT, video)\n",
        "    print(\"Fetching %s => %s\" % (urlpath, cache_path))\n",
        "    data = request.urlopen(urlpath, context=unverified_context).read()\n",
        "    open(cache_path, \"wb\").write(data)\n",
        "  return cache_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrnT3WYLMac9"
      },
      "source": [
        "The following functions allow you to load a video to feed the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZbakrClLnpF"
      },
      "source": [
        "# Utilities to open video files using CV2\n",
        "def crop_center_square(frame):\n",
        "  y, x = frame.shape[0:2]\n",
        "  min_dim = min(y, x)\n",
        "  start_x = (x // 2) - (min_dim // 2)\n",
        "  start_y = (y // 2) - (min_dim // 2)\n",
        "  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(224, 224)):\n",
        "  cap = cv2.VideoCapture(path)\n",
        "  frames = []\n",
        "  try:\n",
        "    while True:\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      frame = crop_center_square(frame)\n",
        "      frame = cv2.resize(frame, resize)\n",
        "      frame = frame[:, :, [2, 1, 0]]\n",
        "      frames.append(frame)\n",
        "      \n",
        "      if len(frames) == max_frames:\n",
        "        break\n",
        "  finally:\n",
        "    cap.release()\n",
        "  return np.array(frames) / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VwK1qm_M9QQ"
      },
      "source": [
        "The `to_gif` function converts a video to a gif."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvPq7i0FM82B"
      },
      "source": [
        "def to_gif(images):\n",
        "  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
        "  imageio.mimsave('./animation.gif', converted_images, fps=25)\n",
        "  return embed.embed_file('./animation.gif')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBvmjVICIp3W"
      },
      "source": [
        "## 3.0 Processing UCF101 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPZ6MA_FNk7z"
      },
      "source": [
        "We get the video list from the dataset and then we print it out for convenience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-QcxdhLIfi2"
      },
      "source": [
        "# Get the list of videos in the dataset.\n",
        "ucf_videos = list_ucf_videos()\n",
        "  \n",
        "categories = {}\n",
        "for video in ucf_videos:\n",
        "  category = video[2:-12]\n",
        "  if category not in categories:\n",
        "    categories[category] = []\n",
        "  categories[category].append(video)\n",
        "print(\"Found %d videos in %d categories.\" % (len(ucf_videos), len(categories)))\n",
        "\n",
        "for category, sequences in categories.items():\n",
        "  summary = \", \".join(sequences[:2])\n",
        "  print(\"%-20s %4d videos (%s, ...)\" % (category, len(sequences), summary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBwRrcBBOC7I"
      },
      "source": [
        "The benefit of using the `fetch_ucf_video` function is that we don't need to download the entire dataset because it downloads a specific sample. \n",
        "\n",
        "Then using `load_video` function, we obtain a sample to use with I3D model. Note that we can use `to_gif` function to visualize the video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0ZvVDruN2nU"
      },
      "source": [
        "# Get a sample video.\n",
        "video_path = fetch_ucf_video(\"v_PlayingViolin_g01_c01.avi\")\n",
        "sample_video = load_video(video_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hASLA90YFPTO"
      },
      "source": [
        "sample_video.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMRduz6JN6vf"
      },
      "source": [
        "to_gif(sample_video)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTPyqD4tPRSf"
      },
      "source": [
        "## 4.0 Loading and using a pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61ZsgYHQNY-z"
      },
      "source": [
        "First, we need to obtain the labels for our model. I3D model was trained Kinectics-400, for that reason we have 400 labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIKTs-KneUfz"
      },
      "source": [
        "# Get the kinetics-400 action labels from the GitHub repository.\n",
        "KINETICS_URL = \"https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt\"\n",
        "with request.urlopen(KINETICS_URL) as obj:\n",
        "  labels = [line.decode(\"utf-8\").strip() for line in obj.readlines()]\n",
        "print(\"Found %d labels.\" % len(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWJJtPtgPb-c"
      },
      "source": [
        "TensorFlow Hub is a repository of trained machine learning models ready for fine-tuning and deployable anywhere. In this tutorial, we work with a Inflated 3D (I3D) Convnet model trained for action recognition on Kinetics-400.\n",
        "\n",
        "The following line loads the model ready to used for predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POf5XgffvXlD"
      },
      "source": [
        "i3d = hub.load(\"https://tfhub.dev/deepmind/i3d-kinetics-400/1\").signatures['default']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjks1JabRJJE"
      },
      "source": [
        "We define a function that allows to obtain the probabilities and show the top-5 predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mTbqA5JGYUx"
      },
      "source": [
        "def predict(sample_video):\n",
        "  # Add a batch axis to the to the sample video.\n",
        "  model_input = tf.constant(sample_video, dtype=tf.float32)[tf.newaxis, ...]\n",
        "\n",
        "  logits = i3d(model_input)['default'][0]\n",
        "  probabilities = tf.nn.softmax(logits)\n",
        "\n",
        "  print(\"Top 5 actions:\")\n",
        "  k=5\n",
        "  for i in np.argsort(probabilities)[::-1][:k]:\n",
        "    print(f\"  {labels[i]:22}: {probabilities[i] * 100:5.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDXgaOD1zhMP"
      },
      "source": [
        "Run the I3D model and print the top-5 action predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykaXQcGRvK4E"
      },
      "source": [
        "predict(sample_video)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyB1J13MPoRv"
      },
      "source": [
        "## 5.0 Predict on own data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHsq0lHXCsD4"
      },
      "source": [
        "We can use any video downloaded from the Internet to predict the action with the I3D model.\n",
        "\n",
        "Let's try with [this video](https://commons.wikimedia.org/wiki/File:End_of_a_jam.ogv) of Patrick Gillett: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-mZ9fFPCoNq"
      },
      "source": [
        "!wget https://upload.wikimedia.org/wikipedia/commons/8/86/End_of_a_jam.ogv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpLmE8rjEbAF"
      },
      "source": [
        "video_path = \"End_of_a_jam.ogv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHZJ9qTLErhV"
      },
      "source": [
        "sample_video = load_video(video_path)[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZNLkEZ9Er-c"
      },
      "source": [
        "to_gif(sample_video)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yskHIRbxEtjS"
      },
      "source": [
        "predict(sample_video)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbRMEiNEP5xd"
      },
      "source": [
        "## 6.0 Activity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXiqOtG2Supv"
      },
      "source": [
        "Now it is your turn. Reuse the code of the section 5.0 to predict the action of the following video: https://github.com/bryanyzhu/tiny-ucf101/raw/master/abseiling_k400.mp4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqSUv6tXIvp3"
      },
      "source": [
        "!wget https://github.com/bryanyzhu/tiny-ucf101/raw/master/abseiling_k400.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHj1suej4K-U"
      },
      "source": [
        "video_path = 'abseiling_k400.mp4'\n",
        "sample_video = load_video(video_path)[:100]\n",
        "to_gif(sample_video)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAJAY2IX4eP4"
      },
      "source": [
        "predict(sample_video)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7x2k-5DPofz"
      },
      "source": [
        "\n",
        "Based on this tutorial and the class, answer the questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgSfM-hF_vRO"
      },
      "source": [
        "1. Is the I3D model a 3d or 2d model?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_0yr_Cyl8YK"
      },
      "source": [
        "**RESPUESTA** I3D es un modelo 3D porque usa kernels de 3 dimensiones, dos espaciales y uno temporal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPWip8Qim0Q_"
      },
      "source": [
        "2. Is the I3D model a trimmed model approach?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCoiN8Og_vRO"
      },
      "source": [
        "**RESPUESTA** Es un modelo que resulta de inflar un modelo convolucional de 2D. Se infla de partir de la idea de repetir el kernel de una misma imagen varias veces, para reutilizar el conocimiento ya aprendido de modelos de imagenes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhMbPo_4_2_d"
      },
      "source": [
        "3. Mention at least one advantages of the I3D over the previous approaches:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvPcwYlK_2_d"
      },
      "source": [
        "**RESPUESTA** Una gran ventaja es que logra aprovechar la información de modelos de imagenes 2D como por ej. ImageNet, en vez de C3D que tiene que aprender desde cero. Por lo tanto, tiene menos números de parámetros y toma menos entrenarlo que C3D. Además logra capturar mejor detalle temporal y espacial que el modelo two-streams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epA2T41D_qti"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}